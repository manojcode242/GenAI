{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMf/nPRPwchqRpc0S443RIw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W3JiJA9n1-mq","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"error","timestamp":1755410655818,"user_tz":-330,"elapsed":4351,"user":{"displayName":"Max","userId":"09006374745954393397"}},"outputId":"6c1ab2a1-5db5-477e-e4eb-8d328b08315b","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","drive  sample_data  virtual-zoom-background-1500-x-1000-1dnfprs7qdlurwup.jpg\n","cp: cannot stat 'drive/My Drive/your_data_folder/': No such file or directory\n","Found 0 images.\n"]},{"output_type":"error","ename":"Exception","evalue":"No IMGS found, check the path.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2995792202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {NUM_IMAGES} images.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Corrected print statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNUM_IMAGES\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Corrected indentation and error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No IMGS found, check the path.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: No IMGS found, check the path."]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt # Corrected import\n","import os\n","import pickle\n","from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n","from tensorflow.keras.layers import Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","\n","\n","\n","#Note : This data set is based on HUMAN Faces\n","class VariationalAutoEncoder():\n","  def __init__(self, input_dim, encoder_conv_filters, encoder_conv_kernel_size, encoder_conv_strides,\n","               decoder_conv_t_filters, decoder_conv_t_kernel_size, decoder_conv_t_strides,\n","               z_dim, use_dropout=False): # Corrected use_batch_norm to use_dropout based on usage\n","    self.name = 'variational_autoencoder'\n","\n","    self.input_dim = input_dim # size of the input img\n","    self.encoder_conv_filters = encoder_conv_filters # encoder conv layers depth\n","    self.encoder_conv_kernel_size = encoder_conv_kernel_size # encoder conv kernel size\n","    self.encoder_conv_strides = encoder_conv_strides # encoder conv strides\n","    self.decoder_conv_t_filters = decoder_conv_t_filters # decoder conv transpose layers depth\n","    self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size # decoder conv kernel size\n","    self.decoder_conv_t_strides = decoder_conv_t_strides # decoder conv strides\n","    self.z_dim = z_dim # dimension of latent space\n","    self.use_dropout = use_dropout # use dropouts or not\n","\n","    self.n_layers_encoder = len(encoder_conv_filters) # nor of encoder conv layers\n","    self.n_layers_decoder = len(decoder_conv_t_filters) # nor of decoder conv transpose layers\n","\n","    self._build()\n","\n","\n","    # BUILD THE FULL VAE MODEL\n","  def _build(self): # Corrected indentation of _build\n","\n","    # ENCODER\n","\n","    # A model that takes an input img and encodes it into the 2D latent space,\n","    # by sampling a point from the normal distribution defined by mu and log_var.\n","\n","    encoder_input = Input(shape=self.input_dim, name='encoder_input')\n","    x = encoder_input\n","\n","    for i in range(self.n_layers_encoder):\n","      conv_layer = Conv2D(filters=self.encoder_conv_filters[i],\n","                          kernel_size=self.encoder_conv_kernel_size[i],\n","                          strides=self.encoder_conv_strides[i],\n","                          padding='same',name='encoder_conv_' + str(i))\n","      x = conv_layer(x)\n","      x = BatchNormalization()(x)\n","      x = LeakyReLU()(x)\n","      if self.use_dropout:\n","        x = Dropout(rate=0.25)(x)\n","\n","    shape_before_flattening = K.int_shape(x)[1:]\n","    x = Flatten()(x)\n","    self.mu = Dense(self.z_dim, name='mu')(x)\n","    self.log_var = Dense(self.z_dim, name='log_var')(x)\n","    # We choose to map to the logarithm of the variance, as this can take any real\n","    # number in the range (-inf, inf) matching the natural output range from a\n","    # NN unit, whereas variance values are always +ve\n","\n","    self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n","\n","    # Now, since we are sampling a random point from an area around mu, the decoder\n","    # must ensure that all pts in the same neighborhodd produce very similar imgs when\n","    # decoded, so that the reconstruction loss remians small.\n","\n","  def sampling(self, args): # Corrected indentation of sampling\n","    mu, log_var = args\n","    epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., stddev=1.)\n","    return mu + K.exp(log_var / 2) * epsilon\n","\n","    # Latent space\n","    encoder_output = Lambda(self.sampling, name='encoder_output')([self.mu, self.log_var]) # Added self. to sampling\n","    self.encoder = Model(encoder_input, encoder_output)\n","\n","    # DECODER\n","\n","    # A model that takes a pt in the latent sapce and decodes it into the original img domain\n","\n","    decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n","    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n","    x = Reshape(shape_before_flattening)(x)\n","\n","    for i in range(self.n_layers_decoder):\n","      conv_t_layer = Conv2DTranspose(filters=self.decoder_conv_t_filters[i],\n","                                     kernel_size=self.decoder_conv_t_kernel_size[i],\n","                                     strides=self.decoder_conv_t_strides[i],\n","                                     padding = 'same', name = 'decoder_conv_t_' + str(i))\n","      x = conv_t_layer(x)\n","      if i < self.n_layers_decoder - 1: # condition for not having an-leakyrelu-dropout at last layer\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU()(x)\n","        if self.use_dropout:\n","          x = Dropout(rate=0.25)(x)\n","      else:\n","        x = Activation('sigmoid')(x)\n","\n","    decoder_output = x # Corrected indentation of decoder_output\n","    self.decoder = Model(decoder_input, decoder_output) # Corrected indentation of self.decoder\n","\n","    ### THE FULL VAE\n","    model_input = encoder_input # Corrected model_input variable name\n","    model_output = self.decoder(encoder_output) # Corrected indentation of model_output\n","\n","    self.model = Model(model_input, model_output) # Corrected indentation of self.model\n","\n","    ## DEFINE THE LOSS FUNCN AND OPITMIZER\n","  def compile(self, learning_rate, reco_loss_factor): # Corrected indentation of compile\n","    self.learning_rate = learning_rate\n","    self.reco_loss_factor = reco_loss_factor # Stored reco_loss_factor\n","\n","    # Binary cross-entropy places havier penalties on predictions at the extremes\n","    # that are badly wrong so it tends to push pixel predictions to the middle of the\n","    # range.This results in less vibrant imgs. for this reason, we use RMSE as the LOSS FUNCN\n","\n","    def vae_r_loss(y_true, y_pred):\n","      r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n","      return self.reco_loss_factor * r_loss # reco_loss_factor ensures balance with the KL divergence loss\n","\n","\n","    # KL divergence term finializes the n/w for encoding observations to mu\n","    # and log_var varibles that differ significantly from the parameters of a\n","    # standard normal distribution, namely mu = 0 and log_var = 0\n","\n","    def kl_loss(y_true, y_pred):\n","      #kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1)\n","      # use the mu and log_var that are passed as arg to this funcn\n","\n","      mu, log_var = self.encoder_mu_log_var(y_true) # get the mu and log_var from the encoder model\n","      kl_loss = -0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis = 1) # Use the mu, log_var from the encoder model\n","      return kl_loss\n","\n","    def vae_loss(y_true, y_pred):\n","      r_loss = vae_r_loss(y_true, y_pred)\n","      kl_loss = kl_loss(y_true, y_pred)\n","      return r_loss + kl_loss\n","\n","    optimizer = Adam(lr=self.learning_rate) # Corrected indentation of optimizer\n","    self.model.compile(optimizer=optimizer, loss=vae_loss, metrics=[vae_r_loss, kl_loss]) # Corrected indentation of self.model.compile\n","\n","\n","# Mount google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# os.mkdri('data/') # Commented out as mkdir is done by cp\n","!ls\n","\n","# Corrected cp command - **PLEASE REPLACE 'drive/My Drive/your_data_folder/' with the actual path to your data folder**\n","!cp -r 'drive/My Drive/your_data_folder/' .\n","\n","\n","from glob import glob\n","\n","DATA_FOLDER = 'data/celeba' # Corrected data folder path\n","IMAGE_FOLDER = 'data/celeba/celeba_dataset' # Corrected image folder path\n","INPUT_DIM = (128, 128, 3)\n","BATCH_SIZE = 32\n","filenames = np.array(glob(os.path.join(IMAGE_FOLDER, '*.jpg')))\n","NUM_IMAGES = len(filenames)\n","LEARNING_RATE = 0.0005 # Corrected variable name\n","R_LOSS_FACTOR = 10000\n","EPOCHS = 10\n","print(f\"Found {NUM_IMAGES} images.\") # Corrected print statement\n","if NUM_IMAGES == 0: # Corrected indentation and error message\n","  raise Exception(\"No IMGS found, check the path.\")\n","\n","\n","# IMPORT Libraries\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator # Corrected indentation\n","\n","data_gen = ImageDataGenerator(rescale=1./255)\n","data_flow = data_gen.flow_from_directory(DATA_FOLDER, target_size = INPUT_DIM[:2], # Corrected target_size\n","                                         batch_size = BATCH_SIZE, shuffle = True,\n","                                         class_mode = 'input')\n","\n","vae = VariationalAutoEncoder(input_dim = INPUT_DIM, # Corrected class name\n","                             encoder_conv_filters = [32, 64, 64, 64],\n","                             encoder_conv_kernel_size = [3, 3, 3, 3],\n","                             encoder_conv_strides = [2, 2, 2, 2],\n","                             decoder_conv_t_filters = [64, 64, 32, 3],\n","                             decoder_conv_t_kernel_size = [3, 3, 3, 3],\n","                             decoder_conv_t_strides = [2, 2, 2, 2],\n","                             z_dim = 200,\n","                             use_dropout = True)\n","\n","vae.encoder.summary()\n","\n","vae.decoder.summary()\n","\n","vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n","\n","checkpoint = ModelCheckpoint('weights_vae.weights.h5', save_weights_only=True) # Added checkpoint definitation\n","def lr_scheduler(epoch):\n","  if epoch < 5:\n","    return 0.0005\n","  else :\n","    return 0.0005 * np.exp(0.1 *(5-epoch)) # Adjusted the lr schedule\n","\n","\n","vae.model.fit(data_flow, shuffle = True, epochs = EPOCHS,\n","                      steps_per_epoch = NUM_IMAGES // BATCH_SIZE, # Corrected variable name\n","                      callbacks = [checkpoint, LearningRateScheduler(lr_scheduler)])\n","\n","# load the model after 50 epochs\n","# vae.load_weights('weights_vae.weights.h5') # Commented out loading weights after 10 epochs\n","\n","vae.model.fit(data_flow, shuffle = True, epochs = 50,\n","                      steps_per_epoch = NUM_IMAGES // BATCH_SIZE, # Corrected variable name\n","                      callbacks = [checkpoint, LearningRateScheduler(lr_scheduler)])\n","\n","vae.model.load_weights('weights_vae.weights.h5')\n","\n","n_to_show = 30\n","znew = np.random.randn(n_to_show, vae.z_dim)\n","reconst = vae.decoder.predict(znew)\n","\n","fig = plt.figure(figsize=(18, 5))\n","fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","for i in range(n_to_show):\n","  ax = fig.add_subplot(3, 10, i+1)\n","  ax.imshow(reconst[i, :, :, :])\n","  ax.axis('off')\n","plt.show() # Corrected indentation of plt.show()\n","\n","\n"]},{"cell_type":"markdown","source":["# Fashion MNIST Data"],"metadata":{"id":"MyU_KZZpoVym"}},{"cell_type":"code","source":["# https://keras.io/api/datasets/fashioni_mnist/\n","\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from keras import layers\n","import matplotlib.pyplot as plt\n","\n","\n","# Load and preprocess fashion MINIST dataset\n","(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n","fashion_mnist = np.concatenate([x_train, x_test], axis=0)\n","fashion_mnist = np.expand_dims(fashion_mnist, axis=-1).astype(\"float32\") / 255\n","\n","\n","# Define the VAE MODEL\n","\n","class VAE(keras.Model):\n","  def __init__(self, encoder, decoder):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n","    self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n","    self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n","\n","  @property\n","  def metrics(self):\n","    return [\n","        self.total_loss_tracker,\n","        self.reconstruction_loss_tracker,\n","        self.kl_loss_tracker,\n","    ]\n","\n","  def train_step(self, data):\n","    with tf.GradientTape() as tape:\n","      mean, log_var, z = self.encoder(data)\n","      reconstruction = self.decoder(z)\n","      reconstruction_loss = tf.reduce_mean(\n","          tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2))\n","      )\n","      kl_loss = -0.5 * (1 + log_var - tf.square(mean) - tf.exp(log_var))\n","      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","      total_loss = reconstruction_loss + kl_loss\n","\n","    grads = tape.gradient(total_loss, self.trainable_weights)\n","    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","    self.total_loss_tracker.update_state(total_loss)\n","    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","    self.kl_loss_tracker.update_state(kl_loss)\n","    return {\n","        \"loss\": self.total_loss_tracker.result(),\n","        \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","        \"kl_loss\": self.kl_loss_tracker.result(),\n","    }\n","\n","\n","# Sampling layer to introduce stochasticity in latent space representation\n","# we are generating one random noise vector per sample in the bacth\n","\n","class Sampling(layers.Layer):\n","  \"\"\"Sampling layer applies reparameterization trick to enable backpropagation.\"\"\"\n","  def call(self, inputs):\n","    mean, log_var = inputs\n","    batch = tf.shape(mean)[0]\n","    dim = tf.shape(mean)[1]\n","    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))  # mean and epsilon must have the same shape so that element-wise operations work correctly\n","    return mean + tf.exp(0.5 * log_var) * epsilon\n","\n","\n","# Define latent_dim\n","latent_dim = 2\n","\n","\n","# Define Encoder\n","\n","encoder_inputs = keras.Input(shape=(28, 28, 1))\n","x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(16, activation=\"relu\")(x)\n","mean = layers.Dense(latent_dim, name=\"mean\")(x)\n","log_var = layers.Dense(latent_dim, name=\"log_var\")(x)\n","z = Sampling()([mean, log_var])\n","encoder = keras.Model(encoder_inputs, [mean, log_var, z], name=\"encoder\")\n","encoder.summary()\n","\n","\n","# Define Decoder\n","\n","latent_inputs = keras.Input(shape=(latent_dim,))\n","x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n","x = layers.Reshape((7, 7, 64))(x)\n","x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n","decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n","decoder.summary()\n","\n","# Instantiate and train the VAE model\n","\n","vae = VAE(encoder, decoder)\n","vae.compile(optimizer=keras.optimizers.Adam())\n","vae.fit(fashion_mnist, epochs=10, batch_size=128)\n","\n","# Function to visualize the latent sapce\n","def plot_latent_space(vae, n=10, figsize=15):\n","  img_size = 28\n","  scale = 0.5\n","  figure = np.zeros((img_size * n, img_size * n))\n","  grid_x = np.linspace(-scale, scale, n)\n","  grid_y = np.linspace(-scale, scale, n)[::-1]\n","\n","  for i, yi in enumerate(grid_y):\n","    for j, xi in enumerate(grid_x):\n","      x_decoded = vae.decoder.predict(np.array([[xi, yi]]))\n","      digit = x_decoded[0].reshape(img_size, img_size)\n","      figure[\n","        i * img_size : (i + 1) * img_size,\n","        j * img_size : (j + 1) * img_size,\n","      ] = digit\n","\n","  plt.figure(figsize=(figsize, figsize))\n","  start_range = img_size // 2\n","  end_range = n * img_size + start_range\n","  pixel_range = np.arrange(start_range, end_range, img_size)\n","  sample_range_x = np.round(grid_x, 1)\n","  sample_range_y = np.round(grid_y, 1)\n","  plt.xticks(pixel_range, sample_range_x)\n","  plt.yticks(pixel_range, sample_range_y)\n","  plt.xlabel(\"z[0]\")\n","  plt.ylabel(\"z[1]\")\n","  plt.imshow(figure, camp=\"Greys_r\")\n","  plt.show()\n","\n","  plot_latent_space(vae)\n","\n","\n","  #NOTE: This kind of model is used for the CHILD TO OLD AGE FACE"],"metadata":{"id":"hXT-e72wlviz","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f1403714-8a4f-4a07-8b81-df18ce1e55e9","executionInfo":{"status":"ok","timestamp":1755417055427,"user_tz":-330,"elapsed":1274093,"user":{"displayName":"Max","userId":"09006374745954393397"}}},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"encoder\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m18,496\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m50,192\u001b[0m │ flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ mean (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ log_var (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ sampling_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n","│ (\u001b[38;5;33mSampling\u001b[0m)          │                   │            │ log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">50,192</span> │ flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ sampling_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>)          │                   │            │ log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m69,076\u001b[0m (269.83 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,076</span> (269.83 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m69,076\u001b[0m (269.83 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,076</span> (269.83 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"decoder\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)           │         \u001b[38;5;34m9,408\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_7              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m289\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,089\u001b[0m (254.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,089</span> (254.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,089\u001b[0m (254.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,089</span> (254.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 167ms/step - kl_loss: 3.0986 - loss: 400.2387 - reconstruction_loss: 397.1401\n","Epoch 2/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 165ms/step - kl_loss: 5.5776 - loss: 331.2611 - reconstruction_loss: 325.6836\n","Epoch 3/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 165ms/step - kl_loss: 5.2892 - loss: 313.8908 - reconstruction_loss: 308.6015\n","Epoch 4/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 168ms/step - kl_loss: 4.8979 - loss: 308.1933 - reconstruction_loss: 303.2953\n","Epoch 5/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 165ms/step - kl_loss: 4.7790 - loss: 304.1121 - reconstruction_loss: 299.3330\n","Epoch 6/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 168ms/step - kl_loss: 4.7444 - loss: 303.5197 - reconstruction_loss: 298.7753\n","Epoch 7/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 165ms/step - kl_loss: 4.7745 - loss: 301.3699 - reconstruction_loss: 296.5955\n","Epoch 8/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 167ms/step - kl_loss: 4.7310 - loss: 300.7516 - reconstruction_loss: 296.0206\n","Epoch 9/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 167ms/step - kl_loss: 4.7239 - loss: 299.6755 - reconstruction_loss: 294.9516\n","Epoch 10/10\n","\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 165ms/step - kl_loss: 4.7418 - loss: 298.5207 - reconstruction_loss: 293.7788\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HMZr5Xa4psb3"},"execution_count":null,"outputs":[]}]}