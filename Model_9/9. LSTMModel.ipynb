{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObV8jdx3RN47POjbSh+zDg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prk8P2-55R7s","outputId":"2c171d4a-36f6-432b-c5cc-0c9a1713557d","executionInfo":{"status":"ok","timestamp":1755600783794,"user_tz":-330,"elapsed":90612,"user":{"displayName":"Max","userId":"09006374745954393397"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 1s/step - accuracy: 0.0443 - loss: 7.2053\n","Epoch 2/5\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 1s/step - accuracy: 0.0610 - loss: 6.4746\n","Epoch 3/5\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 1s/step - accuracy: 0.0933 - loss: 6.1040\n","Epoch 4/5\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 1s/step - accuracy: 0.1057 - loss: 5.8285\n","Epoch 5/5\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m805s\u001b[0m 1s/step - accuracy: 0.1147 - loss: 5.6075\n","Harry looked at the witch and the chamber of snape’s student and ron hastily “but mr the way of the empty time ” said fred harry didn’t worry to see the chamber of a chamber of ron in his arm and george explained as a car he looked ginny ” he said neville\n"]}],"source":[" # hppts://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","import numpy as np\n","\n","\n","# Load and preprocess text\n","def load_data(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","        return text\n","\n","# Load Harry potter book txt\n","file_path = \"hp_1.txt\"\n","text = load_data(file_path).lower()\n","\n","\n","# Tokenization\n","tokenizer = Tokenizer(oov_token=\"<OOV>\") # Out of vocabulary token\n","                                         # If a word not seen during training appears later. it will be replaced with oov\n","                                         # Helps handle unknown words insted of ignoring them\n","tokenizer.fit_on_texts([text]) # analyzes the input txt and creates a word index (mapping of words to unique integers)\n","total_words = len(tokenizer.word_index) + 1 # 0 is usually reserved for padding\n","\n","# Convert txt to sequences\n","input_sequences = []\n","tokens = tokenizer.texts_to_sequences([text])[0] # converts the input txt into a list of nors based on the word index\n","seq_length = 50 # Each input sequence contains 50 words\n","\n","for i in range(seq_length, len(tokens)):\n","    input_sequences.append(tokens[i-seq_length:i + 1])\n","\n","\n","# pad seq and split inputs/targets\n","# after this x will have input and y will have label for those inputs\n","\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n","x, y = input_sequences[:,:-1], input_sequences[:,-1]\n","\n","# One_hot encode the labels, note - there are other ways for\n","# encoding like pre-trained word2vec encoding and so on\n","\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n","\n","# Build the simple LSTM MODEL\n","\n","model = Sequential([\n","    Embedding(input_dim=total_words, output_dim=100, input_length=seq_length), # word embeddings\n","    LSTM(256, return_sequences=True), # LSTM Layer\n","    LSTM(256), # Second LSTM Layer\n","    Dense(total_words, activation='softmax') # Output Layer\n","\n","])\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(x, y, epochs=5, batch_size=128)\n","\n","# Function to generates txt using LSTM\n","def generate_text(seed_text, next_words=50, temperature = 0.7):\n","    for _ in range(next_words):\n","       tokenized_input = tokenizer.texts_to_sequences([seed_text])[0]\n","       tokenized_input = pad_sequences([tokenized_input], maxlen=seq_length, padding='pre')\n","\n","       predicted_probs = model.predict(tokenized_input, verbose=0)\n","       predicted_probs = np.log(predicted_probs) / temperature # Adjust randomness\n","       predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n","       predicted_index = np.random.choice(len(predicted_probs[0]), p=predicted_probs[0])\n","\n","       output_word = tokenizer.index_word.get(predicted_index, \"\")\n","       seed_text += \" \" + output_word\n","\n","    return seed_text\n","\n","\n","# Generate txt using the trained model\n","print(generate_text(\"Harry looked at\", next_words=50, temperature = 0.7))"]},{"cell_type":"code","source":[],"metadata":{"id":"HZJ4maOX5ab8"},"execution_count":null,"outputs":[]}]}