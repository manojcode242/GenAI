{"cells":[{"cell_type":"markdown","metadata":{"id":"77hALBPtzycU"},"source":["# RNN Model CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fuw4zyW2ycOk","outputId":"f251cffd-9d04-4bcf-83eb-5a50387bde75","executionInfo":{"status":"ok","timestamp":1755586385111,"user_tz":-330,"elapsed":1410636,"user":{"displayName":"Max","userId":"09006374745954393397"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 194ms/step - accuracy: 0.0466 - loss: 7.0145\n","Epoch 2/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 194ms/step - accuracy: 0.0868 - loss: 6.2068\n","Epoch 3/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 194ms/step - accuracy: 0.1165 - loss: 5.7271\n","Epoch 4/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 194ms/step - accuracy: 0.1336 - loss: 5.4209\n","Epoch 5/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 194ms/step - accuracy: 0.1469 - loss: 5.1562\n","Epoch 6/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 192ms/step - accuracy: 0.1563 - loss: 4.9189\n","Epoch 7/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 194ms/step - accuracy: 0.1704 - loss: 4.6845\n","Epoch 8/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 195ms/step - accuracy: 0.1812 - loss: 4.4500\n","Epoch 9/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 193ms/step - accuracy: 0.1929 - loss: 4.2359\n","Epoch 10/10\n","\u001b[1m697/697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 192ms/step - accuracy: 0.2084 - loss: 4.0070\n","Harry looked at the door and and ron followed the door and stepped forward and saw it he had been brought up in the air and saw it “bed reach and the sorting hat had been teaching the chamber of secrets chapter the great hall where he was a violent patch of hot\n"]}],"source":[" # hppts://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","import numpy as np\n","\n","\n","# Load and preprocess text\n","def load_data(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","        return text\n","\n","file_path = \"hp_1.txt\" # Ensure u have this file in ur colab\n","text = load_data(file_path).lower()\n","\n","\n","# Tokenization\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","tokenizer = Tokenizer(oov_token=\"<OOV>\") # Out of vocabulary token\n","                                         # If a word not seen during training appears later. it will be replaced with oov\n","                                         # Helps handle unknown words insted of ignoring them\n","tokenizer.fit_on_texts([text]) # analyzes the input txt and creates a word index (mapping of words to unique integers)\n","total_words = len(tokenizer.word_index) + 1 # 0 is usually reserved for padding\n","\n","# Convert txt to sequences\n","input_sequences = []\n","tokens = tokenizer.texts_to_sequences([text])[0] # converts the input txt into a list of nors based on the word index\n","seq_length = 50 # Each input sequence contains 50 words\n","\n","# First seq_length tokens (input): used for training the model\n","# Last token (target): used as the label the model tries to predict.\n","# so total of (50 + 1) in one input_seq idx\n","\n","for i in range(seq_length, len(tokens)):\n","    input_sequences.append(tokens[i-seq_length:i + 1])\n","\n","\n","# pad seq and split inputs/targets\n","# after this x will have input and y will have label for those inputs\n","\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n","x, y = input_sequences[:,:-1], input_sequences[:,-1]\n","\n","# One_hot encode the labels, note - there are other ways for\n","# encoding like pre-trained word2vec encoding and so on\n","\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n","\n","# Build the simple RNN MODEL\n","\n","model = Sequential([\n","    Embedding(input_dim=total_words, output_dim=100, input_length=seq_length), # word embeddings\n","    SimpleRNN(256, return_sequences=False), # RNN Layer\n","    Dense(256, activation='relu'), # Fully Connected Layer\n","    Dense(total_words, activation='softmax') # Output Layer\n","\n","])\n","\n","# 256 in RNN - The nor of hidden units (size of the hidden state vector)\n","# return_sequences=False - the RNN will only return the final hidden state after processing the entire sequence\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(x, y, epochs=10, batch_size=128)\n","\n","# Function to generates txt using RNN\n","def generate_text(seed_text, next_words=50):\n","    for _ in range(next_words):\n","       tokenized_input = tokenizer.texts_to_sequences([seed_text])[0]\n","       # Ensure tokenized_input is a list of lists for pad_sequences\n","       tokenized_input = [tokenized_input] if isinstance(tokenized_input, list) else [[tokenized_input]]\n","       tokenized_input = pad_sequences(tokenized_input, maxlen=seq_length, padding='pre')\n","\n","       predicted_probs = model.predict(tokenized_input, verbose=0)\n","       predicted_index = np.argmax(predicted_probs)\n","       predicted_word = tokenizer.index_word.get(predicted_index, \"<OOV>\") # Use .get() to handle potential missing keys\n","\n","       seed_text += \" \" + predicted_word\n","\n","    return seed_text\n","\n","\n","# Generate txt using the trained model\n","print(generate_text(\"Harry looked at\")) # Here model should guess the next 50 words form the hp_1.txt file and give the output of it"]},{"cell_type":"markdown","metadata":{"id":"HcgbiwChwuCE"},"source":["The model learns local patterns, not long-term dependencies\n","\n","    . RNN struggle with long-range dependencies cause they do not retain info well over long sequences\n","\n","    . This is why the txt seems grammatically ok but lacks deeper context.\n","\n","\n","The model generates phrases on probabilites\n","\n","    .It predicts the most likely nxt word given the past words.\n","\n","    . It does not understand meaning but follows statistical patterns.\n","\n","    . It captures writting style but lacks coherence\n","\n","\n","Word appear logically related but do not form a strong narrative/meaning. The model does not truly \"understand\" the book, it just mimics word usage."]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPd3PWLJ5+VhTBkSz9RV9mA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}