{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/m+l2KEqbnwFkGf3zJRyl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer Pre-Processing Part"],"metadata":{"id":"XOZQMkUNGs95"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gIWV1ymFGcH","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1755690854022,"user_tz":-330,"elapsed":36,"user":{"displayName":"Max","userId":"09006374745954393397"}},"outputId":"3d73f2bb-8a38-4f20-d58e-6072557d2d10"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'hp_1.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-494715688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hp_1.txt\"\u001b[0m \u001b[0;31m# Ensure u have this file in ur colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-494715688.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load and preprocess text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hp_1.txt'"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n","import numpy as np\n","\n","\n","# Load and preprocess text\n","def load_data(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","        return text\n","\n","file_path = \"hp_1.txt\" # Ensure u have this file in ur colab\n","text = load_data(file_path).lower()\n","\n","\n","# Convert txt to sequences\n","input_sequences = []\n","tokens = tokenizer.texts_to_sequences([text])[0] # converts the input txt into a list of nors based on the word index\n","seq_length = 50 # Each input sequence contains 50 words\n","\n","# First seq_length tokens (input): used for training the model\n","# Last token (target): used as the label the model tries to predict.\n","# so total of (50 + 1) in one input_seq idx\n","\n","for i in range(seq_length, len(tokens)):\n","    input_sequences.append(tokens[i-seq_length:i + 1])\n","\n","\n","# pad seq and split inputs/targets\n","# after this x will have input and y will have label for those inputs\n","\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n","x, y = input_sequences[:,:-1], input_sequences[:,-1]\n","\n","# One_hot encode the labels, note - there are other ways for\n","# encoding like pre-trained word2vec encoding and so on\n","\n","y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n","\n","# THE Above entire code is pre-processing part"]},{"cell_type":"markdown","source":["# CORE OF THE TRANSFORMER MODEL"],"metadata":{"id":"acrzkHykGjtA"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, embed_dim, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads # example = 8\n","\n","        self.embed_dim = embed_dim # 512\n","        # embed_dim = dimension of Q, K, and V before splitting into xle heads\n","        # It is same as total dimension of the input embeddings (word embeddings)\n","\n","        self.projection_dim = embed_dim // num_heads # Size of Each Attention Head's Subspace\n","        # Each head gets a samller subspace of the embedding dimension\n","        # ex = 64\n","\n","        # Fully connected (dense) layers that project the input into Q, K, V\n","        # These layers map the input embeddings  to the same embed_dim\n","        # These layers will be reshaped / split later to split across attention heads\n","        # A single large matirx xlcn is more effiecent than many small ones\n","        # GPUs love large matrix xlcn casue they are optimized for ll le computation\n","        # This allows TF/keras to efficiently batch the computation, leaveraging better GPU memory utilization\n","\n","        self.query_dense = Dense(embed_dim) # Q deterimines \"what to focous on\"\n","        self.key_dense = Dense(embed_dim) # K acts as \"labels to be matched with queries\"\n","        self.value_dense = Dense(embed_dim) # V holds the actual info\n","\n","        self.combine_heads = Dense(embed_dim)\n","        # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n","\n","     # Calculating self attention over here\n","    def attention (self, query, key, value):\n","      scores = tf.matmul(query, key, transpose_b=True)\n","      scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # Converting integer to a float32 tensor\n","\n","      # Calculating softmax here\n","      attention_probs = tf.nn.softmax(scores, axis = 1) # how much attention each token should give to other tokens\n","      # The higher the score, the more focus that token gets\n","      # softmax should be applied along keys (i.e, across the last D of the scores matrix)\n","      # each row corresponds to a query token attending to all key tokens.\n","      # This ensures that each query distributes its attention across all keys property\n","      # each row sums to 1\n","\n","      return tf.matmul(attention_probs, value), attention_probs\n","\n","      # x - query, key or value with shape - (batch_size, seq_len, embed_dim)\n","      # batch_size - nor of sequeneces being processed in llle (for bacth processing)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n","        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n","        # the -1 in tf.reshape is a placeholder that tells Tensorflow to automatically\n","        # infer that dimension's value based on the total nor of elements in the tensor\n","        # -1 is replaced by seq_len by tensorflow\n","\n","     # In TF.keras - call(self, inputs) is a standard method used inside Layer subclasses\n","     # to define the forward pass of a NN layer\n","\n","    def call(self, inputs):\n","        query, key, value = inputs\n","        batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n","\n","        query = self.split_heads(self.query_dense(query), batch_size)\n","        key = self.split_heads(self.key_dense(key), batch_size)\n","        value = self.split_heads(self.value_dense(value), batch_size)\n","\n","        attention, _ = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) # Corrected transpose dimensions\n","        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n","        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n","\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n","        # Merges all heads back into a single vector\n","        # (batch_size, seq_len, num_heads, projection_Dim) - (batch_size, seq_len, embed_dim)\n","        return self.combine_heads(concat_attention) # Return the combined attention output\n","\n","class TransformerBlock(Layer):\n","      def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","          super(TransformerBlock, self).__init__()\n","          self.att = MultiHeadAttention(embed_dim, num_heads)\n","          self.ffn = tf.keras.Sequential([\n","              Dense(ff_dim, activation=\"relu\"),\n","              Dense(embed_dim),\n","          ])\n","\n","          # y = (x-mean) / root(variance + epsilon)\n","          # epsilon ensures we never divide by zero\n","          # it is small enough not to affect the result but large enough to prevent instablility\n","          self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","          self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","          self.dropout1 = Dropout(rate)\n","          self.dropout2 = Dropout(rate)\n","\n","      def call(self, inputs, training):\n","        attn_output = self.att([inputs, inputs, inputs])\n","\n","        #Dropout randomly deactivates some neurons during training to reduce overfitting\n","        # ensure dropout is only applied during training, not interfernce(vector)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output) # Residual connection\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output) # Residual connection\n","\n","# Token and position embedding part done over here\n","class TokenAndPositionEmbedding(Layer):\n","    def __init__(self, max_len, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = Embedding(input_dim=max_len, output_dim=embed_dim)\n","        # The embedding layer takes an integer tensor and replaces each integer with an embed_dim-sized vector\n","\n","    def call(self, x):\n","      # the max seq len the model can handle\n","        max_len = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n","        positions = tf.range(start=0, limit=max_len, delta=1) # Generate [0, 1, 2, ...., maxlen=1]\n","        positions = self.pos_emb(positions) # Each position index is mapped of shape (batch_size, maxlen, embed_dim),\n","        x = self.token_emb(x) # Each token ID in x is mapped  to an embedding of shape (batch_size, maxlen, embed_dim)\n","        return x + positions\n","\n","        # x has shape (batch_size, maxlen, embed_dim)\n","        # positions has shape (maxlen, embed_dim)\n","        # But maxlen == seq_len, so positions effectively has shape (seq_len, embed_dim),\n","        # Tensorflow broadcasts positions across batch_size, treating it as if it were (1, seq_len, embed_dim).\n","        # This allows element-wise addition btw x and position"],"metadata":{"id":"5AmErkyZGPMC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model the whole architecture, compile and run the training"],"metadata":{"id":"eBFKi3zbQYKY"}},{"cell_type":"code","source":["# Model parameters\n","embed_dim = 128 # Embedding size for each token\n","num_heads = 4 # Number of attention heads\n","ff_dim = 512 # Hidden layer size in feed forward network inside transformer\n","maxlen = seq_length # Maximum sequence length (50 defined above)\n","\n","# Tokenization\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","tokenizer = Tokenizer(oov_token=\"<OOV>\") # Out of vocabulary token\n","                                         # If a word not seen during training appears later. it will be replaced with oov\n","                                         # Helps handle unknown words insted of ignoring them\n","tokenizer.fit_on_texts([text]) # analyzes the input txt and creates a word index (mapping of words to unique integers)\n","\n","\n","# Calculate total_words based on the tokenizer from the previous cell\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Build the model\n","inputs = tf.keras.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n","\n","x = embedding_layer(inputs)\n","print(x.shape)\n","\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x, training=True)\n","\n","print(x.shape)\n","x = x[:, -1, :]\n","print(x.shape)\n","\n","x= Dense(total_words, activation=\"softmax\")(x)\n","print(x.shape)\n","model = tf.keras.Model(inputs=inputs, outputs=x)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"id":"zV9RC6ilQvHl","colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"status":"error","timestamp":1755690854691,"user_tz":-330,"elapsed":42,"user":{"displayName":"Max","userId":"09006374745954393397"}},"outputId":"22506c45-cea3-43d7-93e9-9239adb4d81d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'text' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3716751822.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                          \u001b[0;31m# If a word not seen during training appears later. it will be replaced with oov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                          \u001b[0;31m# Helps handle unknown words insted of ignoring them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# analyzes the input txt and creates a word index (mapping of words to unique integers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"]}]},{"cell_type":"code","source":["history = model.fit(x, y, batch_size=32, epochs=10)"],"metadata":{"id":"IfqxoCLtX0sn","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"error","timestamp":1755675247781,"user_tz":-330,"elapsed":78,"user":{"displayName":"Max","userId":"09006374745954393397"}},"outputId":"41593d34-258e-42c8-e8c7-5b91055416da"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a real number, not 'NoneType'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-779609258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         msg = (\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UVB8dIELZNCx"},"execution_count":null,"outputs":[]}]}